{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7Tz5vv/NoTz5k7ihW1330",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megha2328/Generative-AI/blob/main/Summarization_fine_tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj29BBGjfkye",
        "outputId": "1459e311-b47c-4552-8af6-27cda41e0846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=c8a7a9dff48fdf18fba29d3208772f82384883e13b1c3b89b7f1b77164aa5d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q4TVDe3Ift60"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI-related papers\n",
        "query='ai OR artificial intelligence OR machine learning'\n",
        "search=arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate) # Find 10 research papers that are most recently submitted\n",
        "\n",
        "# Fetch papers\n",
        "papers=[]\n",
        "client=arxiv.Client()\n",
        "for result in client.results(search):\n",
        "  papers.append({\n",
        "     'published':result.published,\n",
        "     'title':result.title,\n",
        "     'abstract':result.summary,\n",
        "     'categories':result.categories\n",
        "  })\n",
        "\n",
        "  # Convert to Dataframe\n",
        "  df=pd.DataFrame(papers)\n",
        "\n",
        "  pd.set_option('display.max_colwidth',None) # This will ensure that pandas will not truncate any part of the summary even if it is long. It will display the entire summary in the dataframe."
      ],
      "metadata": {
        "id": "GNS6Lo7Cf41U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "soFV4TMGjr-G",
        "outputId": "c0c3a74d-e32e-4f6d-9883-acf5abcfbb52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2025-02-20 18:59:52+00:00   \n",
              "1 2025-02-20 18:59:51+00:00   \n",
              "2 2025-02-20 18:59:42+00:00   \n",
              "3 2025-02-20 18:59:34+00:00   \n",
              "4 2025-02-20 18:59:31+00:00   \n",
              "\n",
              "                                                                                                title  \\\n",
              "0                           LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention   \n",
              "1        Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts   \n",
              "2  Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework   \n",
              "3                             Interpretable Text Embeddings and Text Similarity Explanation: A Primer   \n",
              "4                              Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract  \\\n",
              "0  Large language models (LLMs) have shown remarkable potential in processing\\nlong sequences, yet efficiently serving these long-context models remains\\nchallenging due to the quadratic computational complexity of attention in the\\nprefilling stage and the large memory footprint of the KV cache in the decoding\\nstage. To address these issues, we introduce LServe, an efficient system that\\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\\nunifies different hardware-friendly, structured sparsity patterns for both\\nprefilling and decoding attention into a single framework, where computations\\non less important tokens are skipped block-wise. LServe demonstrates the\\ncompatibility of static and dynamic sparsity in long-context LLM attention.\\nThis design enables multiplicative speedups by combining these optimizations.\\nSpecifically, we convert half of the attention heads to nearly free streaming\\nheads in both the prefilling and decoding stages. Additionally, we find that\\nonly a constant number of KV pages is required to preserve long-context\\ncapabilities, irrespective of context length. We then design a hierarchical KV\\npage selection policy that dynamically prunes KV pages based on query-centric\\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\\nreleased at https://github.com/mit-han-lab/omniserve.   \n",
              "1                                                                  Understanding historical and cultural artifacts demands human expertise and\\nadvanced computational techniques, yet the process remains complex and\\ntime-intensive. While large multimodal models offer promising support, their\\nevaluation and improvement require a standardized benchmark. To address this,\\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\\n266 distinct cultures across 10 major historical regions. Designed for\\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\\nframework to assess AI models' capabilities in classification, interpretation,\\nand historical comprehension. By integrating AI with historical research,\\nTimeTravel fosters AI-powered tools for historians, archaeologists,\\nresearchers, and cultural tourists to extract valuable insights while ensuring\\ntechnology contributes meaningfully to historical discovery and cultural\\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\\nhighlighting their strengths and identifying areas for improvement. Our goal is\\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\\nthat technological advancements contribute meaningfully to historical\\ndiscovery. Our code is available at:\\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.   \n",
              "2                                                                                            Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\\ncapabilities by integrating external knowledge. However, existing benchmarks\\nprimarily focus on simple image-text interactions, overlooking complex visual\\nformats like charts that are prevalent in real-world applications. In this\\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\\nTo semi-automatically generate high-quality evaluation samples, we propose\\nCHARt-based document question-answering GEneration (CHARGE), a framework that\\nproduces evaluation data through structured keypoint extraction, crossmodal\\nverification, and keypoint-based generation. By combining CHARGE with expert\\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\\ndomains from real-world documents. Our evaluation reveals three critical\\nlimitations in current approaches: (1) unified multimodal embedding retrieval\\nmethods struggles in chart-based scenarios, (2) even with ground-truth\\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\\nreleased at https://github.com/Nomothings/CHARGE.git.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Text embeddings and text embedding models are a backbone of many AI and NLP\\nsystems, particularly those involving search. However, interpretability\\nchallenges persist, especially in explaining obtained similarity scores, which\\nis crucial for applications requiring transparency. In this paper, we give a\\nstructured overview of interpretability methods specializing in explaining\\nthose similarity scores, an emerging research area. We study the methods'\\nindividual ideas and techniques, evaluating their potential for improving\\ninterpretability of text embeddings and explaining predicted similarities.   \n",
              "4                                                                                                                                                              Large language models (LLMs) often fail to ask effective questions under\\nuncertainty, making them unreliable in domains where proactive\\ninformation-gathering is essential for decisionmaking. We present ALFA, a\\nframework that improves LLM question-asking by (i) decomposing the notion of a\\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\\nrelevance), (ii) controllably synthesizing attribute-specific question\\nvariations, and (iii) aligning models via preference-based optimization to\\nexplicitly learn to ask better questions along these fine-grained attributes.\\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\\ndataset, composed of 17k real-world clinical interactions augmented with 80k\\nattribute-specific preference pairs of follow-up questions, as well as a novel\\nexpert-annotated interactive healthcare QA task to evaluate question-asking\\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\\nexplicitly guiding question-asking with structured, fine-grained attributes\\noffers a scalable path to improve LLMs, especially in expert application\\ndomains.   \n",
              "\n",
              "                            categories  \n",
              "0  [cs.CL, cs.AI, cs.DC, cs.LG, cs.PF]  \n",
              "1                       [cs.CV, cs.LG]  \n",
              "2                       [cs.AI, cs.CV]  \n",
              "3                [cs.CL, cs.AI, cs.IR]  \n",
              "4                              [cs.CL]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed95a9fc-c93a-4a5a-a2d1-824a4c5ff806\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-02-20 18:59:52+00:00</td>\n",
              "      <td>LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</td>\n",
              "      <td>Large language models (LLMs) have shown remarkable potential in processing\\nlong sequences, yet efficiently serving these long-context models remains\\nchallenging due to the quadratic computational complexity of attention in the\\nprefilling stage and the large memory footprint of the KV cache in the decoding\\nstage. To address these issues, we introduce LServe, an efficient system that\\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\\nunifies different hardware-friendly, structured sparsity patterns for both\\nprefilling and decoding attention into a single framework, where computations\\non less important tokens are skipped block-wise. LServe demonstrates the\\ncompatibility of static and dynamic sparsity in long-context LLM attention.\\nThis design enables multiplicative speedups by combining these optimizations.\\nSpecifically, we convert half of the attention heads to nearly free streaming\\nheads in both the prefilling and decoding stages. Additionally, we find that\\nonly a constant number of KV pages is required to preserve long-context\\ncapabilities, irrespective of context length. We then design a hierarchical KV\\npage selection policy that dynamically prunes KV pages based on query-centric\\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\\nreleased at https://github.com/mit-han-lab/omniserve.</td>\n",
              "      <td>[cs.CL, cs.AI, cs.DC, cs.LG, cs.PF]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-02-20 18:59:51+00:00</td>\n",
              "      <td>Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts</td>\n",
              "      <td>Understanding historical and cultural artifacts demands human expertise and\\nadvanced computational techniques, yet the process remains complex and\\ntime-intensive. While large multimodal models offer promising support, their\\nevaluation and improvement require a standardized benchmark. To address this,\\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\\n266 distinct cultures across 10 major historical regions. Designed for\\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\\nframework to assess AI models' capabilities in classification, interpretation,\\nand historical comprehension. By integrating AI with historical research,\\nTimeTravel fosters AI-powered tools for historians, archaeologists,\\nresearchers, and cultural tourists to extract valuable insights while ensuring\\ntechnology contributes meaningfully to historical discovery and cultural\\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\\nhighlighting their strengths and identifying areas for improvement. Our goal is\\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\\nthat technological advancements contribute meaningfully to historical\\ndiscovery. Our code is available at:\\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.</td>\n",
              "      <td>[cs.CV, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-02-20 18:59:42+00:00</td>\n",
              "      <td>Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework</td>\n",
              "      <td>Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\\ncapabilities by integrating external knowledge. However, existing benchmarks\\nprimarily focus on simple image-text interactions, overlooking complex visual\\nformats like charts that are prevalent in real-world applications. In this\\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\\nTo semi-automatically generate high-quality evaluation samples, we propose\\nCHARt-based document question-answering GEneration (CHARGE), a framework that\\nproduces evaluation data through structured keypoint extraction, crossmodal\\nverification, and keypoint-based generation. By combining CHARGE with expert\\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\\ndomains from real-world documents. Our evaluation reveals three critical\\nlimitations in current approaches: (1) unified multimodal embedding retrieval\\nmethods struggles in chart-based scenarios, (2) even with ground-truth\\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\\nreleased at https://github.com/Nomothings/CHARGE.git.</td>\n",
              "      <td>[cs.AI, cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-02-20 18:59:34+00:00</td>\n",
              "      <td>Interpretable Text Embeddings and Text Similarity Explanation: A Primer</td>\n",
              "      <td>Text embeddings and text embedding models are a backbone of many AI and NLP\\nsystems, particularly those involving search. However, interpretability\\nchallenges persist, especially in explaining obtained similarity scores, which\\nis crucial for applications requiring transparency. In this paper, we give a\\nstructured overview of interpretability methods specializing in explaining\\nthose similarity scores, an emerging research area. We study the methods'\\nindividual ideas and techniques, evaluating their potential for improving\\ninterpretability of text embeddings and explaining predicted similarities.</td>\n",
              "      <td>[cs.CL, cs.AI, cs.IR]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-02-20 18:59:31+00:00</td>\n",
              "      <td>Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning</td>\n",
              "      <td>Large language models (LLMs) often fail to ask effective questions under\\nuncertainty, making them unreliable in domains where proactive\\ninformation-gathering is essential for decisionmaking. We present ALFA, a\\nframework that improves LLM question-asking by (i) decomposing the notion of a\\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\\nrelevance), (ii) controllably synthesizing attribute-specific question\\nvariations, and (iii) aligning models via preference-based optimization to\\nexplicitly learn to ask better questions along these fine-grained attributes.\\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\\ndataset, composed of 17k real-world clinical interactions augmented with 80k\\nattribute-specific preference pairs of follow-up questions, as well as a novel\\nexpert-annotated interactive healthcare QA task to evaluate question-asking\\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\\nexplicitly guiding question-asking with structured, fine-grained attributes\\noffers a scalable path to improve LLMs, especially in expert application\\ndomains.</td>\n",
              "      <td>[cs.CL]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed95a9fc-c93a-4a5a-a2d1-824a4c5ff806')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed95a9fc-c93a-4a5a-a2d1-824a4c5ff806 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed95a9fc-c93a-4a5a-a2d1-824a4c5ff806');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-562dd6d6-7cf6-4c35-a731-dabc24320712\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-562dd6d6-7cf6-4c35-a731-dabc24320712')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-562dd6d6-7cf6-4c35-a731-dabc24320712 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-02-20 18:52:24+00:00\",\n        \"max\": \"2025-02-20 18:59:52+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-02-20 18:52:42+00:00\",\n          \"2025-02-20 18:59:51+00:00\",\n          \"2025-02-20 18:58:10+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Generating $\\u03c0$-Functional Molecules Using STGG+ with Active Learning\",\n          \"Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts\",\n          \"FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Generating novel molecules with out-of-distribution properties is a major\\nchallenge in molecular discovery. While supervised learning methods generate\\nhigh-quality molecules similar to those in a dataset, they struggle to\\ngeneralize to out-of-distribution properties. Reinforcement learning can\\nexplore new chemical spaces but often conducts 'reward-hacking' and generates\\nnon-synthesizable molecules. In this work, we address this problem by\\nintegrating a state-of-the-art supervised learning method, STGG+, in an active\\nlearning loop. Our approach iteratively generates, evaluates, and fine-tunes\\nSTGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We\\napply STGG+AL to the design of organic $\\\\pi$-functional materials, specifically\\ntwo challenging tasks: 1) generating highly absorptive molecules characterized\\nby high oscillator strength and 2) designing absorptive molecules with\\nreasonable oscillator strength in the near-infrared (NIR) range. The generated\\nmolecules are validated and rationalized in-silico with time-dependent density\\nfunctional theory. Our results demonstrate that our method is highly effective\\nin generating novel molecules with high oscillator strength, contrary to\\nexisting methods such as reinforcement learning (RL) methods. We open-source\\nour active-learning code along with our Conjugated-xTB dataset containing 2.9\\nmillion $\\\\pi$-conjugated molecules and the function for approximating the\\noscillator strength and absorption wavelength (based on sTDA-xTB).\",\n          \"Understanding historical and cultural artifacts demands human expertise and\\nadvanced computational techniques, yet the process remains complex and\\ntime-intensive. While large multimodal models offer promising support, their\\nevaluation and improvement require a standardized benchmark. To address this,\\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\\n266 distinct cultures across 10 major historical regions. Designed for\\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\\nframework to assess AI models' capabilities in classification, interpretation,\\nand historical comprehension. By integrating AI with historical research,\\nTimeTravel fosters AI-powered tools for historians, archaeologists,\\nresearchers, and cultural tourists to extract valuable insights while ensuring\\ntechnology contributes meaningfully to historical discovery and cultural\\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\\nhighlighting their strengths and identifying areas for improvement. Our goal is\\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\\nthat technological advancements contribute meaningfully to historical\\ndiscovery. Our code is available at:\\n\\\\url{https://github.com/mbzuai-oryx/TimeTravel}.\",\n          \"Speculative sampling has emerged as an important technique for accelerating\\nthe auto-regressive generation process of large language models (LLMs) by\\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\\npass. While state-of-the-art speculative sampling methods use only a single\\nlayer and a language modeling (LM) head as the draft model to achieve\\nimpressive layer compression, their efficiency gains are substantially reduced\\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\\nframework that optimizes draft candidate selection through vocabulary space\\ncompression. By constraining the draft search to a frequency-prioritized token\\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\\nthe equivalence of the final output distribution. Experiments across multiple\\ndatasets demonstrate an average of 1.12$\\\\times$ speedup over the\\nstate-of-the-art speculative sampling method EAGLE-2.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization of Abstract Column\n",
        "from transformers import pipeline\n",
        "abstract=df['abstract'][0]\n",
        "summarizer=pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summarization=summarizer(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFn2O_T8krON",
        "outputId": "610f2fcb-26d4-4311-92e8-95da4ade772f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization[0]['summary_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YcrKehmmlnxD",
        "outputId": "6f783b81-08d1-401a-c45d-bcb82d86cf62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LServe is an efficient system that speeds up long-sequence LLM serving via hybrid sparse attention. It converts half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJxG5EMVmcYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}